import argparse
import base64
import json
import mimetypes
import os
from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED
import shutil
from pathlib import Path
from typing import Callable, List, Tuple, Optional, Dict, Any
import re
import io
import time
import random

import fitz  # PyMuPDF
import requests
from openpyxl import Workbook
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def is_pdf(path: Path) -> bool:
    return path.is_file() and path.suffix.lower() == ".pdf"


def iter_pdfs(input_path: Path) -> List[Path]:
    if input_path.is_file() and is_pdf(input_path):
        return [input_path]
    elif input_path.is_dir():
        return sorted([p for p in input_path.glob("**/*.pdf") if p.is_file()])
    else:
        return []


def save_embedded_images(doc: fitz.Document, out_dir: Path) -> List[Path]:
    image_paths: List[Path] = []
    for page_index in range(len(doc)):
        page = doc.load_page(page_index)
        images = page.get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            base = doc.extract_image(xref)
            img_bytes = base.get("image")
            ext = base.get("ext", "png")
            name = f"page-{page_index+1}-img-{img_index+1}.{ext}"
            img_path = out_dir / name
            with open(img_path, "wb") as f:
                f.write(img_bytes)
            image_paths.append(img_path)
    return image_paths


def render_full_pages(doc: fitz.Document, out_dir: Path, dpi: int = 200) -> List[Path]:
    page_paths: List[Path] = []
    for page_index in range(len(doc)):
        page = doc.load_page(page_index)
        zoom = dpi / 72.0
        mat = fitz.Matrix(zoom, zoom)
        pix = page.get_pixmap(matrix=mat)
        img_path = out_dir / f"page-{page_index+1}-full.png"
        pix.save(img_path)
        page_paths.append(img_path)
    return page_paths


def to_data_uri(image_path: Path) -> str:
    mime, _ = mimetypes.guess_type(str(image_path))
    if not mime:
        mime = "image/png"
    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("ascii")
    return f"data:{mime};base64,{b64}"


def call_doubao_extract_tables(image_path: Path, api_key: str, base_url: str, model: str) -> Tuple[str, Dict[str, int]]:
    """æ—§ç‰ˆè¡¨æ ¼æå–å‡½æ•°ï¼Œä¿ç•™ä»¥å…¼å®¹"""
    data_uri = to_data_uri(image_path)
    prompt = (
        "è¯·ä»å›¾ç‰‡ä¸­æå–æ‰€æœ‰è¡¨æ ¼ï¼Œä¿è¯è¡¨æ ¼é‡Œæ–‡å­—å†…å®¹å‡†ç¡®æ— è¯¯ï¼Œè¡¨æ ¼ä¸­ä»»ä½•åˆ—ä¸­å¦‚æœæœ‰å¼•å·å°±ç›´æ¥æ›¿æ¢æˆä¸Šè¾¹è¡¨æ ¼é‡Œçš„å†…å®¹ï¼Œä¸¥æ ¼è¾“å‡º JSONï¼Œæ ¼å¼ä¸ºï¼š\n"
        "{\n  \"status\": \"ok\",\n  \"tables\": [ { \"name\": \"Table 1\", \"rows\": [[\"col1\",\"col2\"], [\"...\"]] } ]\n}\n"
        "è¦æ±‚ï¼š\n"
        "- ä¸è¦è¾“å‡ºé™¤ JSON ä¹‹å¤–çš„ä»»ä½•æ–‡æœ¬æˆ–æ ‡è®°\n"
        "- è‹¥æ— è¡¨æ ¼ï¼Œè¾“å‡º {\"status\":\"no_table\",\"tables\":[]}\n"
        "- ä¿ç•™æ•°å­—ã€å°æ•°ã€æ—¥æœŸã€å•ä½ç­‰åŸæ ·ï¼›åˆå¹¶å•å…ƒæ ¼æŒ‰è§†è§‰è¡Œåˆ—å±•å¼€\n"
    )
    url = base_url.rstrip("/") + "/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    body = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": data_uri}},
                    {"type": "text", "text": prompt},
                ],
            }
        ],
        # å…³é—­æ€è€ƒæ¨¡å¼ï¼ŒåŠ å¿«å“åº”å¹¶å»æ‰æ€ç»´é“¾è¾“å‡º
        "thinking": {"type": "disabled"},
    }

    # ä»ç¯å¢ƒè¯»å–å¯é…ç½®è¶…æ—¶ä¸é‡è¯•æ¬¡æ•°
    read_timeout = int(os.getenv("ARK_TIMEOUT", "180"))
    retries = int(os.getenv("ARK_RETRIES", "3"))

    last_err = None
    for attempt in range(retries + 1):
        try:
            # ç¦ç”¨ä»£ç†ï¼ˆå¦‚æœç¯å¢ƒå˜é‡è®¾ç½®äº†ä»£ç†ä½†ä¸å¯ç”¨ï¼Œä¼šå¯¼è‡´è¿æ¥å¤±è´¥ï¼‰
            # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ ARK_USE_PROXY=true æ¥å¯ç”¨ä»£ç†
            proxies = None
            if not os.getenv("ARK_USE_PROXY", "").lower() in ("true", "1", "yes"):
                proxies = {"http": None, "https": None}
            # è¿æ¥è¶…æ—¶å›ºå®š10ç§’ï¼Œè¯»å–è¶…æ—¶å¯é…ç½®
            resp = requests.post(url, headers=headers, json=body, timeout=(10, read_timeout), proxies=proxies)
            resp.raise_for_status()
            j = resp.json()
            break
        except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectTimeout, requests.exceptions.ConnectionError) as e:
            last_err = e
            if attempt < retries:
                # æŒ‡æ•°é€€é¿ + æŠ–åŠ¨
                time.sleep((2 ** attempt) + random.uniform(0, 0.5))
                continue
            raise
        except requests.exceptions.RequestException:
            # å…¶å®ƒè¯·æ±‚å¼‚å¸¸ç›´æ¥æŠ›å‡º
            raise
    # Try to unify text output across possible response shapes
    text = None
    usage = {}
    if isinstance(j, dict):
        if "choices" in j and j["choices"]:
            msg = j["choices"][0].get("message", {})
            content = msg.get("content", [])
            if isinstance(content, list) and content:
                parts = []
                for seg in content:
                    if isinstance(seg, dict):
                        t = seg.get("text")
                        if t:
                            parts.append(t)
                if parts:
                    text = "".join(parts)
            if not text and isinstance(msg.get("content"), str):
                text = msg["content"]
        if not text and j.get("output_text"):
            text = j.get("output_text")
    if not text:
        text = resp.text

    # æå– usage
    if isinstance(j, dict) and isinstance(j.get("usage"), dict):
        u = j.get("usage", {})
        usage = {
            "prompt": int(u.get("prompt_tokens", 0) or 0),
            "completion": int(u.get("completion_tokens", 0) or 0),
            "total": int(u.get("total_tokens", 0) or 0),
        }
    return text, usage


def call_doubao_extract_text(image_path: Path, api_key: str, base_url: str, model: str) -> Tuple[str, Dict[str, int]]:
    """æ–°ç‰ˆæ–‡å­—æå–å‡½æ•°ï¼Œè¯†åˆ«æ‰€æœ‰æ–‡å­—å¹¶æŒ‰å±‚çº§ç»“æ„ç»„ç»‡"""
    data_uri = to_data_uri(image_path)
    prompt = (
        "è¯·ä»”ç»†è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ã€‚\n\n"
        "ã€é€‚ç”¨åœºæ™¯ã€‘ï¼šPPTæ¼”ç¤ºæ–‡ç¨¿ã€ä¹¦æœ¬æ‰«æã€æ•™ææ–‡æ¡£ã€æŠ¥å‘Šæ–‡ä»¶ã€è¡¨æ ¼å›¾ç‰‡ç­‰å„ç±»æ–‡æ¡£ã€‚\n\n"
        "ã€è¯†åˆ«è¦æ±‚ã€‘ï¼š\n"
        "1. å‡†ç¡®è¯†åˆ«æ‰€æœ‰å°åˆ·æ–‡å­—ï¼Œä¿æŒåŸæ–‡å†…å®¹ä¸å˜ï¼Œä¸è¦é—æ¼\n"
        "2. å¿½ç•¥ä»¥ä¸‹å¹²æ‰°å…ƒç´ ï¼šæ°´å°ã€ç›–ç« ã€å°ç« ã€æ‰‹å†™æ‰¹æ³¨ã€æ¶‚é¸¦ã€èƒŒæ™¯è£…é¥°\n"
        "3. æ ¹æ®æ–‡å­—çš„æ ¼å¼ç‰¹å¾åˆ¤æ–­å±‚çº§ç»“æ„ï¼š\n"
        "   - å­—ä½“å¤§å°ã€åŠ ç²—ã€ä½ç½®\n"
        "   - ä¸­æ–‡ç¼–å·ï¼šä¸€ã€äºŒã€ä¸‰... æˆ– ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰... é€šå¸¸æ˜¯å¤§æ ‡é¢˜\n"
        "   - æ•°å­—ç¼–å·ï¼š1. 2. 3. æˆ– (1) (2) (3) é€šå¸¸æ˜¯å­æ ‡é¢˜æˆ–åˆ—è¡¨\n"
        "   - ç¼©è¿›å’Œæ®µè½ç»“æ„\n"
        "4. å¯¹äºè¡¨æ ¼å†…å®¹ï¼Œå¿…é¡»ä¸¥æ ¼è¯†åˆ«ï¼š\n"
        "   - å‡†ç¡®è¯†åˆ«è¡¨æ ¼çš„æ‰€æœ‰è¡Œå’Œåˆ—ï¼Œç¡®ä¿åˆ—æ•°ä¸€è‡´\n"
        "   - æ¯è¡Œçš„å•å…ƒæ ¼æ•°é‡å¿…é¡»ä¸è¡¨å¤´åˆ—æ•°å®Œå…¨ä¸€è‡´\n"
        "   - å•å…ƒæ ¼å†…å®¹è¦å®Œæ•´ï¼Œä¸èƒ½é—æ¼æˆ–æˆªæ–­\n"
        "   - ä¸¥æ ¼æŒ‰ç…§è§†è§‰ä¸Šçš„è¡Œåˆ—å¯¹é½ï¼Œä¸èƒ½é”™ä½\n"
        "   - å¦‚æœæŸå•å…ƒæ ¼ä¸ºç©ºï¼Œç”¨ç©ºå­—ç¬¦ä¸² \"\" è¡¨ç¤ºï¼Œä¸èƒ½çœç•¥è¯¥å•å…ƒæ ¼\n"
        "   - è¡¨æ ¼çš„ rows æ•°ç»„ä¸­ï¼Œç¬¬ä¸€è¡Œå¿…é¡»æ˜¯è¡¨å¤´ï¼Œåç»­è¡Œæ˜¯æ•°æ®è¡Œ\n"
        "5. æŒ‰ç…§ä»ä¸Šåˆ°ä¸‹ã€ä»å·¦åˆ°å³çš„é˜…è¯»é¡ºåºç»„ç»‡å†…å®¹\n\n"
        "ã€è¾“å‡º JSON æ ¼å¼ã€‘ï¼š\n"
        "{\n"
        "  \"status\": \"ok\",\n"
        "  \"content\": [\n"
        "    {\"type\": \"h1\", \"text\": \"ä¸€çº§å¤§æ ‡é¢˜ï¼ˆå¦‚ï¼šä¸€ã€xxx æˆ–é¡µé¢æœ€å¤§æ ‡é¢˜ï¼‰\"},\n"
        "    {\"type\": \"h2\", \"text\": \"äºŒçº§æ ‡é¢˜ï¼ˆå¦‚ï¼š(ä¸€) xxx æˆ– 1. xxxï¼‰\"},\n"
        "    {\"type\": \"h3\", \"text\": \"ä¸‰çº§æ ‡é¢˜ï¼ˆå¦‚ï¼š(1) xxx æˆ–å°èŠ‚æ ‡é¢˜ï¼‰\"},\n"
        "    {\"type\": \"paragraph\", \"text\": \"æ­£æ–‡æ®µè½å†…å®¹ï¼Œå¯ä»¥å¾ˆé•¿...\"},\n"
        "    {\"type\": \"list\", \"items\": [\"åˆ—è¡¨é¡¹1\", \"åˆ—è¡¨é¡¹2\", \"åˆ—è¡¨é¡¹3\"]},\n"
        "    {\"type\": \"table\", \"rows\": [[\"è¡¨å¤´1\",\"è¡¨å¤´2\",\"è¡¨å¤´3\"], [\"æ•°æ®1\",\"æ•°æ®2\",\"æ•°æ®3\"], [\"æ•°æ®4\",\"æ•°æ®5\",\"æ•°æ®6\"]]}\n"
        "    // æ³¨æ„ï¼štable çš„ rows ä¸­ï¼Œæ‰€æœ‰è¡Œçš„åˆ—æ•°å¿…é¡»å®Œå…¨ä¸€è‡´ï¼\n"
        "  ]\n"
        "}\n\n"
        "ã€type ç±»å‹è¯´æ˜ã€‘ï¼š\n"
        "- h1: ä¸€çº§å¤§æ ‡é¢˜ï¼ˆé¡µé¢ä¸»æ ‡é¢˜ã€ç« æ ‡é¢˜ã€\"ä¸€ã€äºŒã€ä¸‰\"ç¼–å·çš„æ ‡é¢˜ï¼‰\n"
        "- h2: äºŒçº§æ ‡é¢˜ï¼ˆèŠ‚æ ‡é¢˜ã€\"(ä¸€)(äºŒ)\"æˆ–\"1. 2. 3.\"ç¼–å·çš„æ ‡é¢˜ï¼‰\n"
        "- h3: ä¸‰çº§æ ‡é¢˜ï¼ˆå°èŠ‚æ ‡é¢˜ã€\"(1)(2)(3)\"ç¼–å·çš„æ ‡é¢˜ï¼‰\n"
        "- paragraph: æ™®é€šæ­£æ–‡æ®µè½ï¼ˆæ— ç¼–å·çš„è¿ç»­æ–‡å­—ï¼‰\n"
        "- list: åˆ—è¡¨é¡¹ï¼ˆå¸¦â—ã€â€¢ã€-ç­‰ç¬¦å·çš„çŸ­æ¡ç›®ï¼Œæˆ–è¿ç»­çš„ç¼–å·çŸ­å¥ï¼‰\n"
        "- table: è¡¨æ ¼æ•°æ®ï¼ˆæœ‰æ˜æ˜¾è¡Œåˆ—ç»“æ„çš„å†…å®¹ï¼‰\n"
        "  * rows æ ¼å¼ï¼š[[\"åˆ—1\",\"åˆ—2\",...], [\"æ•°æ®1\",\"æ•°æ®2\",...], ...]\n"
        "  * é‡è¦ï¼šæ‰€æœ‰è¡Œçš„åˆ—æ•°å¿…é¡»å®Œå…¨ä¸€è‡´ï¼Œä¸è¡¨å¤´åˆ—æ•°ç›¸åŒ\n"
        "  * ç©ºå•å…ƒæ ¼ç”¨ç©ºå­—ç¬¦ä¸² \"\" è¡¨ç¤ºï¼Œä¸èƒ½çœç•¥\n\n"
        "ã€é‡è¦æ³¨æ„äº‹é¡¹ã€‘ï¼š\n"
        "- åªè¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–æ–‡æœ¬ã€è§£é‡Šæˆ– markdown æ ‡è®°\n"
        "- è‹¥å›¾ç‰‡æ— å¯è¯†åˆ«æ–‡å­—ï¼Œè¾“å‡º {\"status\":\"no_text\",\"content\":[]}\n"
        "- ä¿ç•™æ‰€æœ‰æ•°å­—ã€æ—¥æœŸã€å•ä½ã€æ ‡ç‚¹ç¬¦å·åŸæ ·\n"
        "- é•¿æ®µè½ä¿æŒå®Œæ•´ï¼Œä¸è¦æ‹†åˆ†æˆå¤šä¸ª paragraph\n"
        "- ç¼–å·ï¼ˆå¦‚\"ä¸€ã€\"\"(1)\"ï¼‰åº”åŒ…å«åœ¨å¯¹åº”æ ‡é¢˜çš„ text ä¸­\n"
    )
    url = base_url.rstrip("/") + "/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    body = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": data_uri}},
                    {"type": "text", "text": prompt},
                ],
            }
        ],
        # å…³é—­æ€è€ƒæ¨¡å¼ï¼ŒåŠ å¿«å“åº”å¹¶å»æ‰æ€ç»´é“¾è¾“å‡º
        "thinking": {"type": "disabled"},
    }

    read_timeout = int(os.getenv("ARK_TIMEOUT", "180"))
    retries = int(os.getenv("ARK_RETRIES", "3"))

    for attempt in range(retries + 1):
        try:
            # ç¦ç”¨ä»£ç†ï¼ˆå¦‚æœç¯å¢ƒå˜é‡è®¾ç½®äº†ä»£ç†ä½†ä¸å¯ç”¨ï¼Œä¼šå¯¼è‡´è¿æ¥å¤±è´¥ï¼‰
            # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ ARK_USE_PROXY=true æ¥å¯ç”¨ä»£ç†
            proxies = None
            if not os.getenv("ARK_USE_PROXY", "").lower() in ("true", "1", "yes"):
                proxies = {"http": None, "https": None}
            resp = requests.post(url, headers=headers, json=body, timeout=(10, read_timeout), proxies=proxies)
            resp.raise_for_status()
            j = resp.json()
            break
        except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectTimeout, requests.exceptions.ConnectionError) as e:
            if attempt < retries:
                time.sleep((2 ** attempt) + random.uniform(0, 0.5))
                continue
            raise
        except requests.exceptions.RequestException:
            raise

    text = None
    usage = {}
    if isinstance(j, dict):
        if "choices" in j and j["choices"]:
            msg = j["choices"][0].get("message", {})
            content = msg.get("content", [])
            if isinstance(content, list) and content:
                parts = []
                for seg in content:
                    if isinstance(seg, dict):
                        t = seg.get("text")
                        if t:
                            parts.append(t)
                if parts:
                    text = "".join(parts)
            if not text and isinstance(msg.get("content"), str):
                text = msg["content"]
        if not text and j.get("output_text"):
            text = j.get("output_text")
    if not text:
        text = resp.text

    if isinstance(j, dict) and isinstance(j.get("usage"), dict):
        u = j.get("usage", {})
        usage = {
            "prompt": int(u.get("prompt_tokens", 0) or 0),
            "completion": int(u.get("completion_tokens", 0) or 0),
            "total": int(u.get("total_tokens", 0) or 0),
        }
    return text, usage


def parse_model_output_to_content(text: str) -> List[Dict[str, Any]]:
    """è§£ææ¨¡å‹è¾“å‡ºï¼Œè¿”å›ç»“æ„åŒ–å†…å®¹åˆ—è¡¨"""
    content_list: List[Dict[str, Any]] = []
    
    # å°è¯•ç›´æ¥è§£æ JSON
    try:
        data = json.loads(text)
        if isinstance(data, dict) and "content" in data:
            content_list = data["content"] or []
    except Exception:
        pass
    
    # å¦‚æœä¸Šé¢æ²¡æˆåŠŸï¼Œå°è¯•ä»ä»£ç å—ä¸­æå– JSON
    if not content_list:
        fence_json = re.findall(r"```json\s*(.*?)```", text, flags=re.DOTALL | re.IGNORECASE)
        candidates = fence_json or re.findall(r"```\s*(.*?)```", text, flags=re.DOTALL)
        for cand in candidates:
            try:
                data = json.loads(cand)
                if isinstance(data, dict) and "content" in data:
                    content_list = data["content"] or []
                    break
            except Exception:
                continue
    
    # å¦‚æœè¿˜æ²¡æˆåŠŸï¼Œå¯å‘å¼ï¼šæŸ¥æ‰¾ç¬¬ä¸€ä¸ª JSON å¯¹è±¡
    if not content_list:
        m = re.search(r"\{[\s\S]*\}", text)
        if m:
            try:
                data = json.loads(m.group(0))
                if isinstance(data, dict) and "content" in data:
                    content_list = data["content"] or []
            except Exception:
                pass
    
    # éªŒè¯å’Œä¿®å¤è¡¨æ ¼ç»“æ„
    validated_list = []
    for item in content_list:
        if isinstance(item, dict) and item.get("type") == "table":
            rows = item.get("rows", [])
            if rows and len(rows) > 0:
                # ç¡®ä¿æ‰€æœ‰è¡Œéƒ½æ˜¯åˆ—è¡¨æ ¼å¼
                normalized_rows = []
                for row in rows:
                    if isinstance(row, (list, tuple)):
                        normalized_rows.append([str(cell) if cell is not None else "" for cell in row])
                    else:
                        # å¦‚æœæŸè¡Œä¸æ˜¯åˆ—è¡¨ï¼Œè·³è¿‡è¯¥è¡Œ
                        continue
                
                # ç»Ÿä¸€åˆ—æ•°ï¼šä»¥ç¬¬ä¸€è¡Œä¸ºæ ‡å‡†
                if normalized_rows:
                    standard_cols = len(normalized_rows[0])
                    for i, row in enumerate(normalized_rows):
                        # è¡¥é½æˆ–æˆªæ–­åˆ°æ ‡å‡†åˆ—æ•°
                        while len(row) < standard_cols:
                            row.append("")
                        normalized_rows[i] = row[:standard_cols]
                    
                    item["rows"] = normalized_rows
        validated_list.append(item)
    
    # å¦‚æœæ— æ³•è§£æ JSONï¼Œå°è¯•å°†çº¯æ–‡æœ¬æŒ‰è¡Œåˆ†å‰²ä½œä¸ºæ®µè½
    if not validated_list:
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        if lines:
            for line in lines:
                validated_list.append({"type": "paragraph", "text": line})
    
    return validated_list


def parse_markdown_table(md: str) -> List[List[str]]:
    lines = [ln.strip() for ln in md.splitlines() if ln.strip()]
    table_lines = [ln for ln in lines if ln.startswith("|") and ln.endswith("|")]
    if not table_lines:
        return []
    rows: List[List[str]] = []
    for ln in table_lines:
        parts = [p.strip() for p in ln.strip("|").split("|")]
        rows.append(parts)
    # Remove separator line (---)
    rows = [r for r in rows if not all(set(c) <= set("-:") for c in r)]
    return rows


def parse_model_output_to_tables(text: str) -> List[Tuple[str, List[List[str]]]]:
    tables: List[Tuple[str, List[List[str]]]] = []
    # Try JSON first
    try:
        data = json.loads(text)
        if isinstance(data, dict) and "tables" in data:
            for idx, t in enumerate(data["tables"] or []):
                name = t.get("name") or f"Table {idx+1}"
                rows = t.get("rows") or []
                if isinstance(rows, list):
                    tables.append((name, rows))
            if tables:
                return tables
    except Exception:
        # Try to extract JSON from code fences or mixed text
        # ```json ... ``` or ``` ... ```
        fence_json = re.findall(r"```json\s*(.*?)```", text, flags=re.DOTALL | re.IGNORECASE)
        candidates = fence_json or re.findall(r"```\s*(.*?)```", text, flags=re.DOTALL)
        for cand in candidates:
            try:
                data = json.loads(cand)
                if isinstance(data, dict) and "tables" in data:
                    for idx, t in enumerate(data["tables"] or []):
                        name = t.get("name") or f"Table {idx+1}"
                        rows = t.get("rows") or []
                        if isinstance(rows, list):
                            tables.append((name, rows))
                    if tables:
                        return tables
            except Exception:
                continue
        # Heuristic: grab first JSON object-like block
        m = re.search(r"\{[\s\S]*\}", text)
        if m:
            try:
                data = json.loads(m.group(0))
                if isinstance(data, dict) and "tables" in data:
                    for idx, t in enumerate(data["tables"] or []):
                        name = t.get("name") or f"Table {idx+1}"
                        rows = t.get("rows") or []
                        if isinstance(rows, list):
                            tables.append((name, rows))
                    if tables:
                        return tables
            except Exception:
                pass

    # Try Markdown tables
    md_rows = parse_markdown_table(text)
    if md_rows:
        tables.append(("Table 1", md_rows))
        return tables

    # Try CSV-like (simple)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if lines and "," in lines[0]:
        csv_rows: List[List[str]] = [ln.split(",") for ln in lines]
        tables.append(("Table 1", csv_rows))
        return tables

    return tables


def write_tables_to_excel(tables: List[Tuple[str, List[List[str]]]], out_path: Path) -> None:
    wb = Workbook()
    # By default, openpyxl creates a sheet named 'Sheet'; we'll replace it when writing first table
    default_sheet = wb.active
    first = True
    for idx, (name, rows) in enumerate(tables, start=1):
        if first:
            ws = default_sheet
            ws.title = name[:31] or f"Table {idx}"
            first = False
        else:
            ws = wb.create_sheet(title=(name[:31] or f"Table {idx}"))
        for row in rows:
            ws.append([str(cell) if cell is not None else "" for cell in row])
    wb.save(out_path)


def write_aggregated_excel(image_tables: List[Tuple[str, List[Tuple[str, List[List[str]]]]]], out_path: Path) -> None:
    """
    image_tables: list of (image_name, tables_per_image)
    tables_per_image: list of (table_name, rows)
    Writes all tables into a single worksheet, inserting one blank row between images.
    """
    wb = Workbook()
    ws = wb.active
    ws.title = "Sheet1"
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•è¡¨æ ¼æ•°æ®ï¼Œæ·»åŠ å‹å¥½æç¤º
    if not image_tables or all(not tables for _, tables in image_tables):
        ws.append(["âŒ æœªè¯†åˆ«åˆ°ä»»ä½•è¡¨æ ¼"])
        ws.append([""])
        ws.append(["å¯èƒ½çš„åŸå› ï¼š"])
        ws.append(["1. å½“å‰ä½¿ç”¨ embedded æ¨¡å¼ï¼Œä½†PDFä¸­çš„å†…åµŒå›¾ç‰‡ä¸åŒ…å«è¡¨æ ¼"])
        ws.append(["2. å›¾ç‰‡è´¨é‡è¾ƒä½ï¼Œæ¨¡å‹æ— æ³•è¯†åˆ«"])
        ws.append([""])
        ws.append(["ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆï¼š"])
        ws.append(["â€¢ å¦‚æœæ˜¯åŸç”ŸPDFæ–‡æ¡£ï¼Œè¯·åœ¨ .env ä¸­è®¾ç½® ARK_SOURCE=page æˆ– both"])
        ws.append(["â€¢ å¦‚æœæ˜¯æ‰«æç‰ˆPDFï¼Œè¯·æ£€æŸ¥å›¾ç‰‡è´¨é‡æˆ–å°è¯•æé«˜DPI"])
        ws.append([""])
        ws.append(["è¯¦è§ README.md ä¸­çš„å¸¸è§é—®é¢˜éƒ¨åˆ†"])
        wb.save(out_path)
        return
    
    for image_name, tables in image_tables:
        for table_name, rows in tables:
            for row in rows:
                ws.append([str(cell) if cell is not None else "" for cell in row])
        # blank line between images
        ws.append([""])
    wb.save(out_path)


def setup_word_styles(doc: Document) -> None:
    """è®¾ç½® Word æ–‡æ¡£çš„æ ·å¼"""
    styles = doc.styles
    
    # è®¾ç½®æ­£æ–‡æ ·å¼
    try:
        normal_style = styles['Normal']
        normal_style.font.name = 'å¾®è½¯é›…é»‘'
        normal_style._element.rPr.rFonts.set(qn('w:eastAsia'), 'å¾®è½¯é›…é»‘')
        normal_style.font.size = Pt(11)
    except Exception:
        pass
    
    # è®¾ç½®æ ‡é¢˜æ ·å¼
    heading_configs = [
        ('Heading 1', 22, True),
        ('Heading 2', 16, True),
        ('Heading 3', 14, True),
    ]
    
    for style_name, font_size, is_bold in heading_configs:
        try:
            style = styles[style_name]
            style.font.name = 'å¾®è½¯é›…é»‘'
            style._element.rPr.rFonts.set(qn('w:eastAsia'), 'å¾®è½¯é›…é»‘')
            style.font.size = Pt(font_size)
            style.font.bold = is_bold
        except Exception:
            pass


def write_content_to_word(all_content: List[Tuple[str, List[Dict[str, Any]]]], out_path: Path) -> None:
    """
    å°†æ‰€æœ‰é¡µé¢çš„ç»“æ„åŒ–å†…å®¹å†™å…¥ Word æ–‡æ¡£
    all_content: list of (image_name, content_list)
    content_list: list of {"type": "h1"|"h2"|"h3"|"paragraph"|"list"|"table", ...}
    """
    doc = Document()
    setup_word_styles(doc)
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œæ·»åŠ å‹å¥½æç¤º
    if not all_content or all(not content for _, content in all_content):
        doc.add_paragraph("âŒ æœªè¯†åˆ«åˆ°ä»»ä½•æ–‡å­—å†…å®¹")
        doc.add_paragraph("")
        doc.add_paragraph("å¯èƒ½çš„åŸå› ï¼š")
        doc.add_paragraph("1. å½“å‰ä½¿ç”¨ embedded æ¨¡å¼ï¼Œä½†PDFä¸­çš„å†…åµŒå›¾ç‰‡ä¸åŒ…å«æ–‡å­—")
        doc.add_paragraph("2. å›¾ç‰‡è´¨é‡è¾ƒä½ï¼Œæ¨¡å‹æ— æ³•è¯†åˆ«")
        doc.add_paragraph("")
        doc.add_paragraph("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
        doc.add_paragraph("â€¢ å¦‚æœæ˜¯åŸç”ŸPDFæ–‡æ¡£ï¼Œè¯·åœ¨ .env ä¸­è®¾ç½® ARK_SOURCE=page æˆ– both")
        doc.add_paragraph("â€¢ å¦‚æœæ˜¯æ‰«æç‰ˆPDFï¼Œè¯·æ£€æŸ¥å›¾ç‰‡è´¨é‡æˆ–å°è¯•æé«˜DPI")
        doc.save(out_path)
        return
    
    for idx, (image_name, content_list) in enumerate(all_content):
        if not content_list:
            continue
            
        for item in content_list:
            item_type = item.get("type", "paragraph")
            
            if item_type == "h1":
                text = item.get("text", "")
                if text:
                    doc.add_heading(text, level=1)
                    
            elif item_type == "h2":
                text = item.get("text", "")
                if text:
                    doc.add_heading(text, level=2)
                    
            elif item_type == "h3":
                text = item.get("text", "")
                if text:
                    doc.add_heading(text, level=3)
                    
            elif item_type == "paragraph":
                text = item.get("text", "")
                if text:
                    p = doc.add_paragraph(text)
                    
            elif item_type == "list":
                items = item.get("items", [])
                for list_item in items:
                    if list_item:
                        p = doc.add_paragraph(str(list_item), style='List Bullet')
                        
            elif item_type == "table":
                rows = item.get("rows", [])
                if rows and len(rows) > 0:
                    # éªŒè¯å¹¶ç»Ÿä¸€è¡¨æ ¼ç»“æ„ï¼šç¡®ä¿æ‰€æœ‰è¡Œçš„åˆ—æ•°ä¸€è‡´
                    # ä»¥ç¬¬ä¸€è¡Œï¼ˆé€šå¸¸æ˜¯è¡¨å¤´ï¼‰çš„åˆ—æ•°ä¸ºæ ‡å‡†
                    if len(rows) > 0:
                        standard_cols = len(rows[0])
                        # ç»Ÿä¸€æ‰€æœ‰è¡Œçš„åˆ—æ•°
                        normalized_rows = []
                        for row in rows:
                            normalized_row = list(row) if isinstance(row, (list, tuple)) else [str(row)]
                            # å¦‚æœåˆ—æ•°ä¸è¶³ï¼Œç”¨ç©ºå­—ç¬¦ä¸²è¡¥é½
                            while len(normalized_row) < standard_cols:
                                normalized_row.append("")
                            # å¦‚æœåˆ—æ•°è¿‡å¤šï¼Œæˆªæ–­åˆ°æ ‡å‡†åˆ—æ•°
                            normalized_row = normalized_row[:standard_cols]
                            normalized_rows.append(normalized_row)
                        
                        # åˆ›å»ºè¡¨æ ¼
                        table = doc.add_table(rows=len(normalized_rows), cols=standard_cols)
                        table.style = 'Table Grid'
                        
                        # å¡«å……è¡¨æ ¼å†…å®¹
                        for row_idx, normalized_row in enumerate(normalized_rows):
                            for col_idx, cell_text in enumerate(normalized_row):
                                cell = table.rows[row_idx].cells[col_idx]
                                cell.text = str(cell_text) if cell_text is not None else ""
                        
                        # è¡¨æ ¼åæ·»åŠ ç©ºè¡Œ
                        doc.add_paragraph("")
        
        # æ¯ä¸ªå›¾ç‰‡/é¡µé¢ä¹‹é—´æ·»åŠ åˆ†éš”ï¼ˆå¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªï¼‰
        if idx < len(all_content) - 1:
            doc.add_paragraph("")
    
    doc.save(out_path)


def natural_key(name: str):
    """Return a key that sorts strings in human/natural order.
    Example: img2.png < img10.png
    """
    parts = re.split(r"(\d+)", name)
    return [int(p) if p.isdigit() else p.lower() for p in parts]


def process_pdf(
    pdf_path: Path,
    output_root: Path,
    api_key: str,
    base_url: str,
    model: str,
    dpi: int = 200,
    progress_cb: Optional[Callable[[str, dict], None]] = None,
    max_workers: int = 1,
    source_mode: str = "both",
    control_getter: Optional[Callable[[], dict]] = None,
    extract_mode: str = "text",  # "text" è¾“å‡º Wordï¼Œ"table" è¾“å‡º Excel
) -> None:
    doc = fitz.open(str(pdf_path))
    pdf_name = pdf_path.stem
    tmp_img_dir = output_root / "tmp" / pdf_name
    
    # æ ¹æ®æ¨¡å¼é€‰æ‹©è¾“å‡ºç›®å½•
    if extract_mode == "table":
        output_dir = output_root / "excel"
    else:
        output_dir = output_root / "word"
    ensure_dir(tmp_img_dir)
    ensure_dir(output_dir)

    embedded_images: List[Path] = []
    full_pages: List[Path] = []
    if source_mode in ("both", "embedded"):
        embedded_images = save_embedded_images(doc, tmp_img_dir)
    if source_mode in ("both", "page"):
        full_pages = render_full_pages(doc, tmp_img_dir, dpi=dpi)

    total = len(embedded_images) + len(full_pages)
    if progress_cb:
        progress_cb(
            "start",
            {"pdf_name": pdf_name, "total": total, "embedded": len(embedded_images), "pages": len(full_pages)},
        )

    # ä¿æŒæå–é¡ºåºï¼šå…ˆæŒ‰é¡µé¢ç´¢å¼•çš„å†…åµŒå›¾ç‰‡ï¼Œå†æ•´é¡µæ¸²æŸ“ï¼›ä¸å†æŒ‰æ–‡ä»¶åé‡æ–°æ’åº
    images_to_process = embedded_images + full_pages
    # æ„å»ºé¡ºåºæ˜ å°„ï¼Œç¡®ä¿å¹¶å‘å®Œæˆåèšåˆé¡ºåºä»ä¸è¿­ä»£ä¸€è‡´
    order_map = {p.name: idx for idx, p in enumerate(images_to_process)}
    seen = set()
    
    # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†å‡½æ•°
    if extract_mode == "table":
        def _process_one(img_path: Path) -> Tuple[Path, Optional[str], Any]:
            try:
                raw_text, usage = call_doubao_extract_tables(img_path, api_key, base_url, model)
                tables = parse_model_output_to_tables(raw_text)
                if not tables:
                    return img_path, "no_tables", None, usage
                return img_path, None, tables, usage
            except Exception as e:
                return img_path, str(e), None, {}
        no_result_msg = "no_tables"
    else:
        def _process_one(img_path: Path) -> Tuple[Path, Optional[str], Any]:
            try:
                raw_text, usage = call_doubao_extract_text(img_path, api_key, base_url, model)
                content_list = parse_model_output_to_content(raw_text)
                if not content_list:
                    return img_path, "no_content", None, usage
                return img_path, None, content_list, usage
            except Exception as e:
                return img_path, str(e), None, {}
        no_result_msg = "no_content"

    done = 0
    image_results: List[Tuple[str, Any]] = []
    usage_totals = {"prompt": 0, "completion": 0, "total": 0}
    
    def _wait_if_paused():
        if not control_getter:
            return False
        while True:
            ctrl = control_getter() or {}
            if ctrl.get("stop"):
                return True
            if not ctrl.get("paused"):
                return False
            time.sleep(0.5)

    if max_workers and max_workers > 1:
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            imgs_iter = (p for p in images_to_process if p not in seen)
            running = set()
            # åˆå§‹æäº¤
            while len(running) < max_workers:
                if _wait_if_paused():
                    break
                nxt = next(imgs_iter, None)
                if not nxt:
                    break
                running.add(ex.submit(_process_one, nxt))
            # å¤„ç†å¾ªç¯
            while running:
                done_set, _ = wait(running, timeout=0.5, return_when=FIRST_COMPLETED)
                for fut in done_set:
                    running.remove(fut)
                    img_path, err, result_data, usage_delta = fut.result()
                    seen.add(img_path)
                    done += 1
                    if usage_delta:
                        usage_totals["prompt"] += usage_delta.get("prompt", 0) or 0
                        usage_totals["completion"] += usage_delta.get("completion", 0) or 0
                        usage_totals["total"] += usage_delta.get("total", 0) or 0
                    if progress_cb:
                        progress_cb("step", {"pdf_name": pdf_name, "done": done, "total": total, "image": img_path.name, "error": err, "usage": usage_delta})
                    if err:
                        print(f"[WARN] å¤„ç† {img_path.name} å¤±è´¥: {err}")
                    else:
                        print(f"[OK] {pdf_name}: {img_path.name}")
                        if result_data:
                            image_results.append((img_path.name, result_data))
                    # å°è¯•æäº¤ä¸‹ä¸€å¼ 
                    if _wait_if_paused():
                        running.clear()
                        break
                    nxt = next(imgs_iter, None)
                    if nxt:
                        running.add(ex.submit(_process_one, nxt))
    else:
        for img_path in images_to_process:
            if img_path in seen:
                continue
            if _wait_if_paused():
                break
            seen.add(img_path)
            img_path, err, result_data, usage_delta = _process_one(img_path)
            done += 1
            if usage_delta:
                usage_totals["prompt"] += usage_delta.get("prompt", 0) or 0
                usage_totals["completion"] += usage_delta.get("completion", 0) or 0
                usage_totals["total"] += usage_delta.get("total", 0) or 0
            if progress_cb:
                progress_cb("step", {"pdf_name": pdf_name, "done": done, "total": total, "image": img_path.name, "error": err, "usage": usage_delta})
            if err:
                print(f"[WARN] å¤„ç† {img_path.name} å¤±è´¥: {err}")
            else:
                print(f"[OK] {pdf_name}: {img_path.name}")
                if result_data:
                    image_results.append((img_path.name, result_data))

    if progress_cb:
        progress_cb("finish", {"pdf_name": pdf_name, "done": done, "total": total, "usage": usage_totals})

    # æ ¹æ®æ¨¡å¼è¾“å‡ºä¸åŒæ ¼å¼
    image_results_sorted = sorted(image_results, key=lambda x: order_map.get(x[0], 10**9))
    if extract_mode == "table":
        excel_out = output_dir / f"{pdf_name}.xlsx"
        write_aggregated_excel(image_results_sorted, excel_out)
    else:
        word_out = output_dir / f"{pdf_name}.docx"
        write_content_to_word(image_results_sorted, word_out)

    # æ¸…ç†ä¸´æ—¶å›¾ç‰‡ç›®å½•
    try:
        shutil.rmtree(tmp_img_dir)
    except Exception:
        pass


def main():
    parser = argparse.ArgumentParser(description="ä» PDF/å›¾ç‰‡ä¸­è¯†åˆ«æ–‡å­—ï¼Œè‡ªåŠ¨åˆ†çº§æ ‡é¢˜ï¼Œè¾“å‡º Word æ–‡æ¡£")
    parser.add_argument("--input", required=True, help="PDF æ–‡ä»¶æˆ–åŒ…å« PDF çš„ç›®å½•è·¯å¾„")
    parser.add_argument("--output", default="output", help="è¾“å‡ºæ ¹ç›®å½•ï¼Œé»˜è®¤ output")
    parser.add_argument("--model", default="doubao-seed-1-6-vision-250815", help="æ¨¡å‹å")
    parser.add_argument("--dpi", type=int, default=200, help="æ•´é¡µæ¸²æŸ“ DPIï¼Œé»˜è®¤ 200")
    parser.add_argument("--source", choices=["both", "embedded", "page"], default=os.getenv("ARK_SOURCE", "both"), help="å›¾ç‰‡æ¥æºï¼šboth/embedded/page")
    args = parser.parse_args()

    api_key = os.getenv("ARK_API_KEY")
    base_url = os.getenv("ARK_BASE_URL")
    if not api_key or not base_url:
        raise RuntimeError(
            "ç¼ºå°‘ Ark API é…ç½®ä¿¡æ¯ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY ä¸ ARK_BASE_URL"
        )

    input_path = Path(args.input)
    output_root = Path(args.output)
    ensure_dir(output_root)

    pdfs = iter_pdfs(input_path)
    if not pdfs:
        raise RuntimeError(f"æœªæ‰¾åˆ° PDFï¼š{input_path}")

    print(f"å‘ç° {len(pdfs)} ä¸ª PDFï¼Œå¼€å§‹å¤„ç†â€¦â€¦")
    workers = int(os.getenv("ARK_WORKERS", "1"))
    for pdf in pdfs:
        print(f"å¤„ç†: {pdf}")
        process_pdf(
            pdf,
            output_root,
            api_key,
            base_url.rstrip("/"),
            args.model,
            dpi=args.dpi,
            progress_cb=None,
            max_workers=max(1, workers),
            source_mode=args.source,
        )
    print("âœ… å¤„ç†å®Œæˆï¼Word æ–‡æ¡£å·²ç”Ÿæˆåˆ° output/word/ ç›®å½•")


def process_images(
    image_paths: List[Path],
    batch_name: str,
    output_root: Path,
    api_key: str,
    base_url: str,
    model: str,
    progress_cb: Optional[Callable[[str, dict], None]] = None,
    max_workers: int = 1,
    control_getter: Optional[Callable[[], dict]] = None,
    extract_mode: str = "text",  # "text" è¾“å‡º Wordï¼Œ"table" è¾“å‡º Excel
) -> None:
    # æ ¹æ®æ¨¡å¼é€‰æ‹©è¾“å‡ºç›®å½•
    if extract_mode == "table":
        output_dir = output_root / "excel"
    else:
        output_dir = output_root / "word"
    ensure_dir(output_dir)

    # ç›´æ¥ä½¿ç”¨ä¸Šä¼ çš„å›¾ç‰‡è·¯å¾„è¿›è¡Œå¤„ç†ï¼Œä¸å†å¤åˆ¶æˆ–ä¿å­˜
    # å¤šå›¾æ‰¹å¤„ç†æŒ‰è‡ªç„¶æ–‡ä»¶åé¡ºåºæ’åºï¼Œç¡®ä¿ img2 åœ¨ img10 ä¹‹å‰
    images_to_process: List[Path] = sorted(list(image_paths), key=lambda p: natural_key(p.name))
    order_map = {p.name: idx for idx, p in enumerate(images_to_process)}

    total = len(images_to_process)
    if progress_cb:
        progress_cb("start", {"pdf_name": batch_name, "total": total, "embedded": total, "pages": 0})

    # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†å‡½æ•°
    if extract_mode == "table":
        def _process_one(img_path: Path) -> Tuple[Path, Optional[str], Any, Dict[str, int]]:
            try:
                raw_text, usage = call_doubao_extract_tables(img_path, api_key, base_url, model)
                tables = parse_model_output_to_tables(raw_text)
                if not tables:
                    return img_path, "no_tables", None, usage
                return img_path, None, tables, usage
            except Exception as e:
                return img_path, str(e), None, {}
    else:
        def _process_one(img_path: Path) -> Tuple[Path, Optional[str], Any, Dict[str, int]]:
            try:
                raw_text, usage = call_doubao_extract_text(img_path, api_key, base_url, model)
                content_list = parse_model_output_to_content(raw_text)
                if not content_list:
                    return img_path, "no_content", None, usage
                return img_path, None, content_list, usage
            except Exception as e:
                return img_path, str(e), None, {}

    done = 0
    image_results: List[Tuple[str, Any]] = []
    seen = set()
    usage_totals = {"prompt": 0, "completion": 0, "total": 0}
    
    def _wait_if_paused():
        if not control_getter:
            return False
        while True:
            ctrl = control_getter() or {}
            if ctrl.get("stop"):
                return True
            if not ctrl.get("paused"):
                return False
            time.sleep(0.5)

    if max_workers and max_workers > 1:
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            imgs_iter = (p for p in images_to_process if p not in seen)
            running = set()
            while len(running) < max_workers:
                if _wait_if_paused():
                    break
                nxt = next(imgs_iter, None)
                if not nxt:
                    break
                running.add(ex.submit(_process_one, nxt))
            while running:
                done_set, _ = wait(running, timeout=0.5, return_when=FIRST_COMPLETED)
                for fut in done_set:
                    running.remove(fut)
                    img_path, err, result_data, usage_delta = fut.result()
                    seen.add(img_path)
                    done += 1
                    if usage_delta:
                        usage_totals["prompt"] += usage_delta.get("prompt", 0) or 0
                        usage_totals["completion"] += usage_delta.get("completion", 0) or 0
                        usage_totals["total"] += usage_delta.get("total", 0) or 0
                    if progress_cb:
                        progress_cb("step", {"pdf_name": batch_name, "done": done, "total": total, "image": img_path.name, "error": err, "usage": usage_delta})
                    if not err and result_data:
                        image_results.append((img_path.name, result_data))
                    if _wait_if_paused():
                        running.clear()
                        break
                    nxt = next(imgs_iter, None)
                    if nxt:
                        running.add(ex.submit(_process_one, nxt))
    else:
        for img_path in images_to_process:
            if img_path in seen:
                continue
            if _wait_if_paused():
                break
            seen.add(img_path)
            img_path, err, result_data, usage_delta = _process_one(img_path)
            done += 1
            if usage_delta:
                usage_totals["prompt"] += usage_delta.get("prompt", 0) or 0
                usage_totals["completion"] += usage_delta.get("completion", 0) or 0
                usage_totals["total"] += usage_delta.get("total", 0) or 0
            if progress_cb:
                progress_cb("step", {"pdf_name": batch_name, "done": done, "total": total, "image": img_path.name, "error": err, "usage": usage_delta})
            if not err and result_data:
                image_results.append((img_path.name, result_data))

    if progress_cb:
        progress_cb("finish", {"pdf_name": batch_name, "done": done, "total": total, "usage": usage_totals})

    # æ ¹æ®æ¨¡å¼è¾“å‡ºä¸åŒæ ¼å¼
    image_results_sorted = sorted(image_results, key=lambda x: order_map.get(x[0], 10**9))
    if extract_mode == "table":
        excel_out = output_dir / f"{batch_name}.xlsx"
        write_aggregated_excel(image_results_sorted, excel_out)
    else:
        word_out = output_dir / f"{batch_name}.docx"
        write_content_to_word(image_results_sorted, word_out)


if __name__ == "__main__":
    main()
